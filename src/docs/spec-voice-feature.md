# Voice Dictation to Structured Tasks: OpenAI vs Azure Workflow Report

## Introduction  
Telepathic is a cross-platform to-do app built with .NET MAUI, and we are adding a **voice-driven planning feature**. This feature will let users tap a button, speak about their plans or day in a free-form manner, and have the app automatically extract **projects and tasks** from that rambling dictation using AI. Under the hood, this involves **speech transcription** and **language understanding** via OpenAI services, with support across Android, iOS, macOS, and Windows. Because the heavy processing is cloud-based, we need to design a robust workflow that captures audio, transcribes it accurately, and uses OpenAI (or comparable services) to parse the text into structured to-do items. 

The challenge is that user speech will be **unstructured and conversational** – e.g. *“We have a birthday party for Billy tomorrow, so we really need to get a haircut and figure out a present before then.”* Users likely won’t explicitly say “this is a project” or “here is a task.” The system must infer actionable items (like “get a haircut” or “buy a present”) and their context (in this case, related to *Billy’s birthday party*). We will explore how to implement this workflow with an emphasis on OpenAI’s tools (like Whisper for speech-to-text and GPT-4 for language understanding), while also comparing alternatives such as Azure Cognitive Services for speech. Key considerations include whether to stream audio in real-time or process it after recording, the capabilities of OpenAI’s Whisper (especially for streaming vs. batch transcription), how Azure’s speech recognition and diarization might offer advantages, and strategies for prompting OpenAI to return clean, structured task data from messy input. The report also covers technical implementation tips, potential risks (accuracy, latency, costs, etc.), and further research areas.

## Workflow Overview  
The high-level workflow for the voice dictation feature consists of the following steps (from capturing audio to saving tasks):

1. **Voice Capture:** The user taps a microphone button in the Telepathic app’s UI to begin a voice session. This opens a modal dialog indicating recording is in progress. The app uses the device microphone to **capture the user’s speech** continuously until the user stops or submits. (.NET MAUI can use libraries like *Plugin.Maui.Audio* for cross-platform audio recording.)  
2. **Transcription:** The recorded audio is sent to a speech recognition service to be transcribed into text. We need to decide between using **OpenAI’s Whisper API** or another service (like Azure Speech). The output at this stage is a raw text transcript of everything the user said.  
3. **AI Parsing (OpenAI GPT):** The transcript text is then sent to an OpenAI language model (via the existing `Microsoft.Extensions.AI` integration) with a carefully crafted prompt. The model’s job is to **analyze the transcript and extract structured data** – identifying any *projects* or contexts mentioned and the *tasks* that need to be done. The result should be a structured list of projects and their tasks (or a flat task list with contextual grouping), filtering out any irrelevant chatter.  
4. **Review UI:** The app displays the AI-generated projects/tasks in the modal for the user to review and edit. Each extracted project and its tasks can be shown as a list (or perhaps as editable text fields). The user can make corrections, delete items that aren’t actually tasks, or add anything the AI missed. They then confirm by tapping “Save.”  
5. **Saving to To-Do List:** Upon saving, the app commits the new projects and tasks into the user’s to-do list (cloud-backend or local database as appropriate) and closes the modal. The user can then see these items in their main task list as usual.

Each of these steps involves specific technical considerations and options, which we discuss in detail below.

## Audio Capture and Transcription Approaches  

### Real-Time Streaming vs. Post-Recording Transcription  
One fundamental decision is **whether to transcribe the audio in real-time (streaming)** as the user speaks, or to **record the full audio first and transcribe it afterward** (batch processing). Real-time transcription would display partial text as the person is speaking, much like modern voice assistants or dictation systems, whereas batch processing would wait until the user finishes speaking, then produce the full transcript at once.

- **Streaming (Real-Time) Transcription:** Using streaming, the app could provide immediate feedback by showing the transcript progressively. This requires a speech recognition service that supports receiving audio in chunks (e.g., via WebSocket or a SDK) and returning partial results on the fly. Azure Cognitive Services’ Speech-to-Text is designed for this – it can **stream audio and return incremental transcriptions** in real time. This could improve UX by letting users see what’s being captured (and possibly correct via voice or stop if misheard). However, implementing streaming adds complexity: we need to maintain an open stream, handle partial results, and possibly send these to OpenAI continuously or accumulate them. OpenAI’s Whisper **API** currently does *not* support true streaming input – it expects a complete audio file for transcription and returns the full result after processing ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=For%20the%20real,time%20processing)) ([Possible to use for real-time / streaming tasks? · openai whisper · Discussion #2 · GitHub](https://github.com/openai/whisper/discussions/2#:~:text=)). There isn’t an official OpenAI endpoint yet that maintains a continuous session for audio (as of early 2025). Thus, to do streaming with Whisper, one would have to manually chop the audio into small segments (e.g. 1-second chunks) and send them one by one, which is non-trivial and could lead to slight delays or boundary issues ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=For%20the%20real,time%20processing)). Given this limitation, if real-time transcription is a priority, Azure’s speech SDK is a strong candidate.  
- **Batch (After Recording) Transcription:** In this simpler approach, the app records the entire utterance (perhaps up to a certain time limit), then sends the audio data in one request to a transcription service (OpenAI or otherwise). The user would not see any text until the processing is done, but since the typical utterance might be on the order of tens of seconds, a short wait (a few seconds for transcription) may be acceptable. Batch transcription is straightforward with OpenAI’s Whisper API: you send the audio file (supported formats include WAV, MP3, WebM, etc.) and get back the full transcript text. For a one-shot dictation of a day’s plan, this approach is likely sufficient and easier to implement. It aligns well with the fact that OpenAI Whisper’s API **only accepts complete files (up to 25 MB each) and does not handle live streams** ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=,AI%20Speech%20batch%20transcription%20API)). In summary, **transcribing after recording is recommended** for simplicity and because it allows leveraging Whisper’s excellent accuracy without complicated workarounds. We can still keep the UI interactive (e.g., showing a “Transcribing…” spinner) without showing partial text. Once the transcription returns, we immediately feed it to the next AI step and then show the results to the user.

Given the current tools, **transcribe-first-then-analyze** is likely the better route. It minimizes integration complexity and leverages the strengths of OpenAI’s models. Streaming can be revisited in the future if OpenAI introduces a real-time Whisper API or if we integrate Azure’s streaming for a more instantaneous feel. In either case, the overall workflow of capturing audio, transcribing, then parsing to tasks remains the same.

### Using OpenAI Whisper API for Transcription  
OpenAI’s Whisper is a state-of-the-art speech recognition model known for its high accuracy and language support. OpenAI provides a cloud API for Whisper (since 2023) that makes it easy to use in our app. Key points about using Whisper for this feature:

- **Accuracy and Quality:** Whisper is widely regarded as extremely accurate in transcribing conversational speech. In one evaluation, OpenAI’s transcription had a word error rate (WER) of only ~7.6%, outperforming both Google’s and Azure’s speech-to-text services on the same test ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision)). It excels at understanding casual or accented speech and even can handle multiple languages. For our use case – which may include informal phrasing and potentially mixed language (if a user throws in a non-English name or phrase) – Whisper’s robustness is a big plus. User reports have noted that *“Whisper is more superior in terms of accuracy and quality”* than Azure’s standard model in side-by-side comparisons ([Does Azure use OpenAI Whisper model to perform Speech to Text ? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1335424/does-azure-use-openai-whisper-model-to-perform-spe#:~:text=Doing%20a%20side%20by%20side,terms%20of%20accuracy%20and%20quality)). This means it should capture the user’s words very closely to what they actually said, reducing the chance of missing a task due to a transcription mistake. 
- **Language and Punctuation:** Whisper supports **57 languages** and is designed to include punctuation and casing in the transcript for readability ([Does Azure use OpenAI Whisper model to perform Speech to Text ? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1335424/does-azure-use-openai-whisper-model-to-perform-spe#:~:text=July%2018th%20at%20Microsoft%20Inspire%2C,creates%20transcripts%20with%20enhanced%20readability)). So if the user speaks in English (or another supported language), the returned text will have sentences with periods, commas, etc., making it easier for GPT to parse meaning. For example, the audio *“…we really need to get a haircut and figure out a present before then.”* might come back as text exactly like that sentence. This is useful because run-on, unpunctuated text can confuse the language model. Whisper’s output quality (especially in the latest versions) will preserve as much structure as possible.  
- **API Usage:** To use Whisper via OpenAI API, our cloud backend (or even directly from the app if we choose) would make a POST request to OpenAI’s audio transcription endpoint with the audio file. The API accepts common audio formats (mp3, wav, m4a, etc.), so using *Plugin.Maui.Audio* we can likely capture audio in a format Whisper accepts (for example, as a WAV or MP3 stream). The response will be JSON containing the transcribed text. This integration can be done with raw HTTP calls or via OpenAI’s .NET SDK. Since our app already uses `Microsoft.Extensions.AI`, we might have built-in support for calling OpenAI’s transcription if that library wraps it; if not, a direct call is straightforward. We should ensure to **limit audio length or size** to what the API allows – OpenAI Whisper API has a file size limit of **25 MB per request** ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=,AI%20Speech%20batch%20transcription%20API)) (and no hard duration limit aside from that). This is generous (roughly 2+ hours of audio in highly compressed format), so typical daily dictations (likely 30 seconds to a few minutes at most) are well within bounds. If a user did speak for an extremely long time (tens of minutes) we might have to chunk the audio and transcribe in pieces, but that’s an edge case. Each transcription call costs *$0.006 per minute* of audio (per OpenAI pricing), which is relatively low ([API model whisper - Real cost - OpenAI Developer Forum](https://community.openai.com/t/api-model-whisper-real-cost/469816#:~:text=API%20model%20whisper%20,So%20I)) – e.g., a 30-second note is <$0.0003.  
- **Latency:** Using Whisper API will introduce some latency as the audio file is uploaded and processed. Whisper models are large, but OpenAI’s hosted service is optimized – transcription is usually quite fast, often near real-time for short clips. For instance, a ~10 second audio might transcribe in a second or two. If the user speaks for 60 seconds, we might expect a few seconds of processing delay. This is usually acceptable, but we will want to provide feedback (like a spinner or “Analyzing…” message) in the UI while waiting for the AI steps. We should also consider network latency (mobile uplink for the audio file and downlink for results), which could add a couple of seconds depending on connection. In sum, the user might wait e.g. 3–5 seconds after finishing speaking to see the tasks extracted – hopefully an acceptable trade-off for the magic of automated organization.

In conclusion, **OpenAI Whisper API** is well-suited for our transcription needs if we are okay with post-recording (non-streaming) transcription. It offers top-tier accuracy that will help the later parsing step. The main drawback is lack of real-time feedback; however, given that our end goal is to show structured tasks (not necessarily the raw transcript to the user), waiting and showing final results might be fine. Next, we’ll consider Azure’s speech capabilities as an alternative or complement, especially if we decide real-time or speaker identification features are important.

### Azure Speech Service for Real-Time Transcription (Comparison)  
Microsoft Azure Cognitive Services includes a **Speech-to-Text service** that could be an alternative for the transcription step. Azure’s speech recognition has some distinct features that are worth comparing against Whisper:

- **Real-Time Streaming Support:** Azure’s SDK and APIs are built with streaming in mind. Using the Azure Speech SDK for .NET (which is available via NuGet and works on all the platforms .NET MAUI targets), we can feed audio from the microphone and receive transcribed text continuously. This means we could implement the voice capture such that as the user is speaking, we get intermediate results and perhaps update the UI. Azure’s service can emit interim results with partial words and then final results for each spoken sentence or pause. This would enable a **more interactive dictation experience**. By contrast, as noted, Whisper API doesn’t natively stream; to emulate it, one would have to manually implement chunking and reassemble text, which Azure does out-of-the-box ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=For%20the%20real,time%20processing)). If UX research shows users want to see their speech recognized live, Azure has an edge here. 
- **Accuracy and Language Coverage:** Azure’s speech-to-text models have improved steadily and support **over 140 languages and dialects** ([Choosing between the OpenAI Whisper and Azure Speech models - Azure AI Services Video Tutorial | LinkedIn Learning, formerly Lynda.com](https://www.linkedin.com/learning/azure-ai-for-developers-azure-ai-speech/choosing-between-openai-whisper-vs-azure-speech-models#:~:text=language%20complexity%2C%20and%20domain%20specificity,or%20for%20applications%20where%20you%E2%80%A6)), far more than Whisper’s 57. For mainstream languages like English, Spanish, etc., Azure’s accuracy is very high, though community evaluations suggest Whisper often still has an advantage in word error rate ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision)). In one test, Azure’s WER was ~14.7% vs. Whisper’s 7.6% on challenging phrases ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision)). This doesn’t mean Azure is always worse – on clean audio, Azure can be very accurate (likely in the 5-10% WER range, approaching human level). Additionally, Azure allows *customization* (e.g. adding a **phrase list** of uncommon words to bias the transcription). For our to-do use case, accuracy is critical but we might not have domain-specific jargon to bias. Whisper’s general-purpose accuracy for informal speech might still win out. **Trade-off:** Whisper might transcribe a rambling sentence more verbatim, whereas Azure might make minor mistakes. However, Azure’s accuracy is probably “good enough” in most cases, especially since the next step (GPT) can often work around minor transcription errors by context. 
- **Speaker Diarization:** Azure’s speech service can perform **speaker diarization**, i.e. distinguishing different speakers in the audio and labeling segments with speaker IDs ([Real-time diarization quickstart - Speech service - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization#:~:text=In%20this%20quickstart%2C%20you%20run,particular%20part%20of%20transcribed%20speech)). This is useful in meeting transcripts or any multi-speaker scenario. In our current feature, the user is presumably the only speaker (they’re dictating their own thoughts), so diarization isn’t needed. However, imagine a scenario where the user records a quick conversation or a meeting to generate tasks – Azure could label which sentences came from, say, the user vs. their colleague. Whisper’s API does *not* provide speaker labels natively (the open-source Whisper model has no built-in diarization; one would have to combine it with another model or library to separate speakers). So, Azure has an advantage for any multi-speaker audio. For now, we assume a single speaker (the user).  
- **Integration and Ecosystem:** Being a Microsoft service, Azure Speech integrates smoothly with .NET. The `Microsoft.CognitiveServices.Speech` SDK can be used in a MAUI app (with the proper native dependencies) to capture from the mic and stream to the cloud. This avoids manually handling HTTP calls or file I/O – you get an event with recognized text. On the other hand, since we already have OpenAI in our stack, adding Azure means using two different cloud services (OpenAI for GPT and Azure for STT). It’s certainly doable, but we’d manage two APIs and auth keys instead of one. There is also an **Azure OpenAI** service, which in preview now includes Whisper as well ([Does Azure use OpenAI Whisper model to perform Speech to Text ? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1335424/does-azure-use-openai-whisper-model-to-perform-spe#:~:text=Thanks%20for%20reaching%20out%20to,an%20exact%20date%20for%20you)). Interestingly, Azure OpenAI’s Whisper is basically the same model as OpenAI’s, just hosted on Azure infrastructure with Azure security benefits. But it has the same limitation of 25MB file limit and no streaming in that mode ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=,AI%20Speech%20batch%20transcription%20API)). So using Azure OpenAI Whisper wouldn’t give streaming; you’d use *either* Azure’s normal Speech-to-Text or Whisper, not a magical combination of both (they are separate offerings). If we were already on Azure for everything, we might choose one of those, but since we want to prioritize OpenAI approach, we likely stick to OpenAI’s API or possibly Azure’s standard STT.

To summarize the comparison, the table below highlights key differences:

| **Feature**               | **OpenAI Whisper API** (OpenAI Cloud)                         | **Azure Speech-to-Text** (Cognitive Service)                |
|---------------------------|--------------------------------------------------------------|-------------------------------------------------------------|
| **Real-time Transcription** | ❌ *Not natively streaming.* Accepts only complete files; real-time requires custom chunking workaround ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=For%20the%20real,time%20processing)). | ✅ *Yes.* Designed for streaming; SDK returns partial & live results out-of-the-box. |
| **Accuracy**             | Excellent on conversational speech (low WER). Outperformed Azure in one head-to-head test (7.6% vs 14.7% error) ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision)). | Very good accuracy (continuously improving). Slightly higher error rate in some cases, but still high-quality for most inputs. |
| **Language Support**     | ~57 languages, auto-detects and transcribes with punctuation. | 140+ languages/dialects supported ([Choosing between the OpenAI Whisper and Azure Speech models - Azure AI Services Video Tutorial | LinkedIn Learning, formerly Lynda.com](https://www.linkedin.com/learning/azure-ai-for-developers-azure-ai-speech/choosing-between-openai-whisper-vs-azure-speech-models#:~:text=language%20complexity%2C%20and%20domain%20specificity,or%20for%20applications%20where%20you%E2%80%A6)), with auto-detection and punctuation. |
| **Speaker Diarization**  | Not provided in API (single speaker assumed per request).     | Available (can label different speakers in multi-speaker audio) ([Real-time diarization quickstart - Speech service - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization#:~:text=In%20this%20quickstart%2C%20you%20run,particular%20part%20of%20transcribed%20speech)). |
| **Integration in .NET**  | Use via HTTP REST API (or OpenAI .NET SDK). Microsoft.Extensions.AI supports OpenAI endpoints. | Use via Azure Speech SDK (NuGet). Native streaming support, but separate from OpenAI GPT integration. |
| **Security & Privacy**   | Data goes to OpenAI’s cloud (OpenAI promises not to train on it by default for API). | Data to Azure (with enterprise-grade security, optional no-log settings). Possibly easier compliance if already using Azure. |
| **Cost**                 | ~$0.006 per minute of audio ([API model whisper - Real cost - OpenAI Developer Forum](https://community.openai.com/t/api-model-whisper-real-cost/469816#:~:text=API%20model%20whisper%20,So%20I)) (about $0.36/hour). Pay-as-you-go, no free tier beyond trial. | ~$1-2 per hour for standard STT (pricing varies by region/volume). Azure offers 5 hours free per month and enterprise discounts. |

Both services have their merits. **Recommended approach:** Given that we already use OpenAI for the parsing step and given Whisper’s strong accuracy, using **OpenAI’s Whisper API for transcription** is a solid choice for an initial implementation. The simplicity of recording then transcribing fits our one-shot use case. If we later decide that live feedback is crucial, we can explore Azure’s streaming – possibly even a hybrid: use Azure to show interim transcript, but still send the final result to Whisper for a “second pass” to get the best accuracy before sending to GPT. That might be overkill, but it’s an idea if needed. For now, assume we’ll do **batch transcription with Whisper**. 

#### Technical Notes on .NET MAUI Audio Recording  
Regardless of which transcription service we use, capturing high-quality audio in a cross-platform way is essential. *Plugin.Maui.Audio* is a community library that supports audio playback and recording in .NET MAUI. Using this, we can start recording when the user taps the mic button. The plugin likely provides a stream or file output of the recorded audio. We should record in a format that balances quality and size – e.g., 16 kHz mono WAV or an MP3. Whisper performs well even on lower-fidelity audio, so we don’t need stereo 48kHz (which would be larger to upload). A reasonable approach is to record to a temporary file, then once the user stops, pass that file to the Whisper API. This avoids holding too much audio data in memory. We must also handle platform-specific permissions (microphone permission on iOS/Android, etc.) which the plugin or MAUI Essentials can help with. On iOS, we might need to add usage descriptions in Info.plist; on Android, request RECORD_AUDIO permission. 

One more consideration: **background noise** and **silence detection**. If the user pauses while thinking, Whisper will still transcribe silence or filler words if any. We may want to trim long silences or use a *voice activity detection* to auto-stop recording after a period of silence to mark the end of dictation. Azure’s SDK has a mode to automatically end recognition on silence; with Whisper, we’d have to decide when to stop. Perhaps a manual button or a timeout (e.g., stop if 5 seconds of silence). Keeping recordings reasonably short will also help with faster turnaround.

Now that we have a game plan for getting the spoken words into text, the next step is the heart of the feature: extracting structured tasks and projects from that text using OpenAI’s language model.

## Parsing the Transcript into Projects and Tasks with OpenAI GPT  

Once we have the transcript of the user’s speech, we need to transform that free-form text into the structured to-do data that Telepathic will use. This is a natural language understanding problem well-suited for **OpenAI’s GPT models**. We’ll leverage the GPT (likely GPT-4 or GPT-3.5 Turbo) via the OpenAI API (or Azure OpenAI) to parse the text. Several design points arise here:

### Prompt Design for Task Extraction  
Crafting the right **prompt** (or system message) is crucial so that the model knows exactly what to do: ignore irrelevant chatter, identify all actionable tasks, determine if there is a project/context, and format the output in a clear structure. We should provide the model with instructions like: *“You are an assistant that extracts to-do items from a user’s dictation. The user will speak informally about their plans or day. Your job is to list all actionable tasks mentioned, and group them by project or theme if applicable. Ignore any statements that don’t represent tasks that need doing.”* We might also give an example or two in the prompt for clarity (few-shot prompting), unless we use a very capable model that can do zero-shot well.

For example, a possible prompt could be:

> **System/Instruction:** You are an AI assistant that helps organize tasks. The user will give you a rambling description of their plans, including things they need to do. Extract all the *tasks* that the user needs to accomplish, and group them by *project* or context if one is mentioned. A "project" here could be an event or theme (like a birthday party or a work project) that the tasks belong to. If no clear project is mentioned, tasks can stand alone. For each project, list its tasks. Only include concrete tasks (things that one could put on a to-do list), and omit any commentary or irrelevant details that are not actionable.  
> **Output format:** Provide the output in a structured JSON format with two top-level keys: "projects" and "tasks". "projects" should be a list of objects, each with a "name" and a "tasks" list. "tasks" (if any) can be a list of standalone task descriptions not tied to a project. Ensure the JSON is valid.  
> **Example:**  
> _Input:_ "There’s a birthday party for Alice coming up, I should buy a gift and maybe bake a cake. Also, the car’s oil needs changing soon."  
> _Output:_ `{ "projects": [ { "name": "Alice's Birthday Party", "tasks": ["Buy a gift for Alice", "Bake a cake"] } ], "tasks": ["Change car oil"] }`  
> Now, here is the actual user dictation:  
> **User:** *{transcript goes here}*

This prompt (especially with an example) guides the model on what we expect. It tells it to drop extraneous info (like “coming up” or commentary about things being fun, etc.) and just focus on the tasks. It also explicitly asks for structured JSON output, which makes it easier for our app to consume without error. OpenAI’s models are usually quite capable of following such instructions. In fact, OpenAI recently introduced **Structured Output** support where you can provide a JSON Schema and the model will *guarantee* the response matches it ([Using Structured Outputs to Generate JSON responses with OpenAI | by Sebastian Jensen | Medialesson | Medium](https://medium.com/medialesson/using-structured-outputs-to-generate-json-responses-with-openai-e01f591b740f#:~:text=In%20some%20scenarios%2C%20receiving%20a,or%20producing%20invalid%20enum%20values)). We could potentially use that feature via the API to enforce the output format strictly (which is helpful to avoid the model accidentally adding extra text). In our .NET environment, the Azure.AI.OpenAI SDK (or the OpenAI .NET SDK) might allow us to specify a function or schema for the model. For instance, we could define a schema for a function “ExtractTasks” that returns a list of projects and tasks. This would leverage the function calling capability of GPT-4 to directly give us a machine-readable result.

Regardless of method, the key is to ensure **relevant vs. irrelevant conversation** is separated. Users will often mention things that are not tasks. For example, in “We have a birthday party for Billy tomorrow, so we need to get a haircut and figure out a present before then,” the phrase about having a birthday party is context – arguably not a task itself (unless the user specifically wanted a reminder about the event). The tasks are the haircut and buying a present. A good prompt will lead the AI to output something like:

- Project: “Billy’s Birthday Party” (since it’s an event context)  
  - Task: “Get a haircut before the party”  
  - Task: “Find a present for Billy”  

If the user’s speech is more disjointed, the model should still list all tasks. For instance, if a user rambles through multiple topics: *“The garage is a mess; at some point I should clean it. Also, next week is the team meeting, I might prepare slides. And oh, I have to buy groceries for dinner.”* – the output could be tasks under a "Team Meeting" project (with “Prepare slides”) and standalone tasks “Clean the garage” and “Buy groceries for dinner.” The irrelevant bit might be "the garage is a mess" (complaint) but it implies a task "clean the garage", which the model should capture. We will need to test and refine the prompt with examples to ensure it captures implied tasks too. GPT-4 is generally excellent at reading context and can infer tasks even if not phrased as explicit “I need to…”, but we must be cautious that it doesn’t overreach and invent tasks that were not mentioned. We should instruct it not to fabricate tasks beyond what the user said, even if they seem logical (e.g., not adding “Buy cake ingredients” unless the user hinted at it).

The output formatting is worth deciding on: **JSON vs. Markdown List.** JSON is easiest for the app to parse. However, if for some reason we wanted to directly display the model’s output in the UI, a nicely formatted Markdown with bullet points could be used. For example, the model could respond with something like:

- **Billy’s Birthday Party**  
  - Get a haircut before the party  
  - Figure out a present for Billy  
- **(General)**  
  - Change the car’s oil

This is human-readable, but parsing it back into app data is more error-prone than JSON. Given that we will present the results in a custom UI anyway (with edit/delete options), using JSON internally is safer. We can always format it for display ourselves. 

In fact, a great approach is to have the model respond with JSON and *not show that JSON to the user directly*, but our code will interpret it and populate the UI elements. The `Microsoft.Extensions.AI` or Azure OpenAI SDK might even have a helper to directly map the JSON to a C# object if using the structured output function. A similar pattern is seen in a project that parses meeting transcripts into tasks for Notion: the developer prompted GPT with instructions to *“return the tasks in JSON format as a list of dictionaries”* with specific keys ([From Meeting Notes to Notion Tasks: AI Project Manager - Cohorte Projects](https://www.cohorte.co/blog/from-meeting-notes-to-notion-tasks-ai-project-manager#:~:text=client%20%3D%20OpenAI,transcript)). This ensured the output was immediately usable by their app to update a tasks table. We can follow that pattern, specifying keys like `"name"` and `"tasks"` for projects.

### Model Choice (GPT-4 vs GPT-3.5)  
Which OpenAI model to use? GPT-3.5 Turbo is faster and cheaper, and often sufficient for extraction tasks, especially with a well-crafted prompt or examples. GPT-4 is more reliable for following complex instructions and understanding subtle context, but it’s slower and costlier. In a real product, we might experiment. Since users can edit the output before saving, a small mistake by the AI is not catastrophic (they can correct it). We might start with GPT-3.5 Turbo to keep response time low (it might return an answer in under a second for a short transcript, whereas GPT-4 might take a couple of seconds and uses more compute). However, GPT-4 might better handle very convoluted speech or ensure it adheres to format. 

One possible strategy is to use GPT-3.5 if the transcript is short/simple, and GPT-4 if it’s long/complex or if 3.5 results were unsatisfactory in testing. This can optimize costs. But for now, we can plan with GPT-4 for best quality, and then evaluate if 3.5 can match it.

### Putting it Together: End-to-End Flow  
With transcription and parsing figured out, the end-to-end cloud flow is: 
- The app records audio, then either the app or a backend server sends that audio to the OpenAI Whisper API. 
- We get back text. Then we send a ChatCompletion request to OpenAI with the prompt + transcript. 
- The model returns structured tasks data (as JSON text). 
- We parse that JSON into our project/task objects and display them.

This could all be done client-side (the app calls OpenAI directly). However, storing API keys in a client app is risky (they could be extracted). A safer architecture is to have our own backend service (e.g., an Azure Function or a minimal Web API) that holds the OpenAI API key. The app would upload the audio to our server, the server does the calls to OpenAI and returns the structured data. This adds a bit of backend work but secures the key and allows central logging or tweaking of prompts. Since Telepathic is cloud-based, we likely have some infrastructure to support this.

On the client UI, after the user finishes dictating, we might display the transcript briefly or directly show the extracted tasks. It could be useful to show the original transcript in the modal as reference (perhaps collapsed or smaller text) so the user knows what the AI heard. However, since the tasks are editable, maybe the transcript isn’t needed unless the user is curious or if the AI output is confusing. To keep it simple, we might just show *“We found these tasks:”* and list them. The user can trust but verify the list.

## Implementation Considerations and Additional Research  

### Workflow and Technical Implementation Notes  
- **Modal UI & Interaction:** The voice capture modal should clearly indicate recording state (e.g., a waveform or a blinking red dot). A **stop button** or “done” button will let the user finish recording. Alternatively, auto-stop on silence can be implemented. After that, show a loading indicator (“Transcribing and analyzing...”) while the AI processes. Then smoothly transition to the results view within the modal. This results view can list each project and its tasks. Perhaps each task has a text box so user can edit text, and a trash icon to delete, and each project name is editable as well. We should consider cases: if the AI found no tasks (maybe the user just mused without any actionable item), we should handle that (e.g., “No tasks identified. You can try again.”). 
- **Error Handling:** What if the transcription fails (network error) or comes back garbled, or the GPT API fails/rate-limits? We should handle those gracefully: maybe show a message “Sorry, I couldn’t understand. Please try again.” and allow retry. Logging such failures is important to improve the system. 
- **OpenAI Rate Limits & Throughput:** If our user base grows, we have to consider the number of audio transcriptions and GPT calls. Both are relatively heavy operations. Whisper API can handle quite a lot, but if many users dictate at once, we must ensure we don’t hit rate limits. We might need to queue requests or upgrade the account plan accordingly. Caching isn’t really applicable since each input is unique. 
- **Privacy and Data Storage:** Audio data is sensitive. OpenAI’s policy for the API is that they don’t use data for training by default (unlike the free ChatGPT service). We should still inform users that audio is sent to the cloud for processing. If any PII is spoken (names, etc.), it will be in the transcript and possibly in the tasks (like “Buy present for Billy” contains a name). We should handle that data according to our privacy policy (likely it’s just user’s own data in their account). If we do keep transcripts for debugging, we must secure them. As an extra layer, we could consider running Whisper locally on device (to avoid sending audio out), but currently Whisper models are too large/heavy to run in real-time on typical mobile devices, and quality would suffer on smaller models. So cloud is the practical choice. 
- **Diarization & Multi-Speaker Future:** As mentioned, currently we assume one speaker. If we later allowed recording a conversation (say a team meeting to extract tasks), Azure’s conversation transcription service might be useful. It can label speakers as “Speaker 1, Speaker 2” and even provide real-time meeting transcription with each speaker’s turn ([Real-time diarization quickstart - Speech service - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization#:~:text=In%20this%20quickstart%2C%20you%20run,particular%20part%20of%20transcribed%20speech)). We would then need GPT to attribute tasks to the right person. This is an advanced scenario (almost like meeting minutes and action items). While not in initial scope, our architecture (transcribe -> GPT) could extend to it. We’d prompt GPT to include who’s responsible for each task if identifiable. There are examples of using GPT for meeting action items extraction ([How I Use ChatGPT to Transform Messy Meeting Notes into Actionable Tasks (And Save 5 Hours Per Week) | by God of Prompt | Medium](https://medium.com/@godofpromptai/how-i-use-chatgpt-to-transform-messy-meeting-notes-into-actionable-tasks-and-save-5-hours-per-d64fb4378af8#:~:text=and%20expectations%20moving%20forward.%20,critical%20to%20the%20project%E2%80%99s%20progress)) ([How I Use ChatGPT to Transform Messy Meeting Notes into Actionable Tasks (And Save 5 Hours Per Week) | by God of Prompt | Medium](https://medium.com/@godofpromptai/how-i-use-chatgpt-to-transform-messy-meeting-notes-into-actionable-tasks-and-save-5-hours-per-d64fb4378af8#:~:text=,to%20all%20meeting%20participants%20and)), which show how to include responsible parties and deadlines. For now, Telepathic likely doesn’t need that complexity, since it’s a personal to-do list (the app user is presumably responsible for all tasks they dictate). 
- **Use of Azure OpenAI vs OpenAI Direct:** If we already use Azure for other parts of Telepathic’s backend, we might consider using **Azure OpenAI Service** for both the Whisper and GPT models. Azure OpenAI provides the same models but in Azure’s ecosystem. Whisper was announced in 2024 for Azure OpenAI (in preview) ([Does Azure use OpenAI Whisper model to perform Speech to Text ? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1335424/does-azure-use-openai-whisper-model-to-perform-spe#:~:text=Thanks%20for%20reaching%20out%20to,an%20exact%20date%20for%20you)). The advantage would be consolidation (all calls go to Azure endpoints) and possibly data residency control or enterprise security. The disadvantage is that Azure OpenAI often lags a bit in model availability and might require specific approvals. If we have access, it could be fine. For our purposes, using OpenAI’s public API is straightforward. Microsoft.Extensions.AI likely can be configured for either OpenAI or Azure OpenAI (since it mentions an `IChatClient` abstraction that can be Azure or local ([Introducing Microsoft.Extensions.AI Preview - Unified AI Building Blocks for .NET - .NET Blog](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/#:~:text=benefiting%20consumers%20immediately))). So it’s more about operational preference. The **technical implementation** of calling the model (prompts, etc.) would remain the same.

### Risks and Challenges  
Implementing this feature introduces some risks and challenges that we should be mindful of:

- **Transcription Errors:** If the speech recognizer mistranscribes something important, the downstream parsing might miss a task or create a wrong one. For example, if the user said “get a haircut,” but background noise made it transcribe as “get a harpoon,” the GPT might be very confused or list a bizarre task. Whisper’s high accuracy mitigates this, but no model is perfect. Our safety net is the user review step – the user can spot any weird output and correct or delete it. We should encourage them to verify. Perhaps showing the original transcript (even faintly) can help them trace why a task appeared. In testing, we should evaluate common failure modes (different accents, fast talkers, etc.). Whisper tends to handle accents well, but if a user uses a mix of languages, the prompt to GPT might need to allow for that (e.g., user says a task in Spanish – do we keep it in Spanish in tasks? Probably yes, keep whatever language the user used for that content).
- **Over- or Under-Inclusion of Tasks:** GPT might sometimes **hallucinate** tasks that the user only implied or didn’t say. Conversely, it might overlook a task that was phrased indirectly. We will need to tune prompts and possibly give examples to avoid these issues. Setting the model’s *temperature* to 0 (fully deterministic and focused) can help it stick to extraction without creative additions. We likely want a low temperature for this task extraction query, to minimize imaginative outputs. In cases where the user’s dictation is long and touches many topics, GPT might also need a large context window. If a user really rambles (say transcribes to several hundred or thousand words), a GPT-3.5 4k-token model can handle it. If they somehow went longer (like a 20-minute monologue), we might need GPT-4 8k or even 32k. That’s probably beyond reasonable use; we can impose a reasonable limit on recording length for practicality (like maybe 2-3 minutes max for a “daily” plan, which is easily handled by standard models).
- **Latency and User Experience:** The chain of audio -> text -> GPT means at minimum two network calls sequentially. If each takes a couple seconds, the user waits, say, ~4-6 seconds. This is generally okay, but if either service is slow, it could be longer. We should measure and possibly optimize (e.g., stream the audio in as we record if using Azure, or perhaps send the transcription and start the GPT call in parallel if we had partial results – though that’s complex). If using only Whisper batch, we have to wait for full transcript anyway. As a backup, we could consider a *timeout* and if Whisper API is taking too long, maybe fall back to Azure or an error message. Ensuring the app doesn’t feel frozen is important – always show some activity indicator and maybe fun messaging (“Telepathic is thinking…”) to keep user engaged.
- **API Costs:** Each usage will incur OpenAI costs (transcription + completion). While per use it’s small, if we have thousands of users doing this daily, it adds up. We should monitor usage. One way to reduce cost is to try using a smaller/cheaper model for GPT (like gpt-3.5) if it works well, or to minimize the prompt tokens (we can keep the prompt concise and just include necessary context, since prompt tokens also count toward billing). The **good news**: a typical dictation might be, say, 100 tokens transcript + prompt ~ hundred or two tokens + output ~ few tens of tokens. Even GPT-4 at $0.03/1K tokens, this is <$0.01 per use. Whisper $0.006/minute might be similarly <$0.01 per use. So it’s not too expensive at moderate scale, but we should keep an eye on high-volume scenarios.
- **User Trust and Correction:** Some users may not trust AI to get it right. They might double-check everything or feel frustrated if it misinterprets. To build trust, we should be transparent (maybe a note like “AI-generated tasks from your note – please review and edit before saving”). Over time, if the feature consistently saves time, users will embrace it. But a few bad experiences could turn them away. Thus, QA testing on varied examples is important. We might gather some sample “daily dictations” from potential users to see how the system handles them.
- **Maintenance and Updates:** OpenAI models and APIs are evolving. We should stay updated on new features. For example, if OpenAI releases a streaming Whisper endpoint or a multimodal GPT that directly accepts audio, that could simplify or improve things. (OpenAI has begun offering voice capabilities in ChatGPT apps, but that’s not yet an API for developers as of this writing.) Also, if new models appear that are better or cheaper, we should be ready to switch. The design we chose is modular (one service for ASR, one for NLU), so we can swap components as needed (for instance, using Google’s API as an alternative if needed, or even allow user to choose engine in settings if we expose that).

### Additional Research Areas  
During this research, a few additional areas came up that we might explore further, even if not implementing immediately:

- **Leveraging OS-Level Speech APIs:** iOS and Android both have built-in speech recognition (Siri dictation, Google Speech) which apps can use offline in some cases. For example, iOS 17+ can do on-device dictation. Using these could potentially allow real-time transcription without cloud cost. However, their integration and output quality may vary, and we wouldn’t easily integrate with GPT in one step. Also, prompting those engines to output structured results is not possible – they only give text. We would still need GPT. Given our cross-platform goal and existing OpenAI integration, sticking to a single cross-platform API (like Whisper) may be simpler, but it’s worth noting for future if offline mode or reducing cloud dependency becomes important.
- **Semantic Understanding vs Keyword:** We chose an LLM (GPT) for parsing because of its superior ability to understand context. An alternative could be a more deterministic approach, like using regex or keyword spotting to find verbs (“need to”, “should”) etc. However, those tend to miss a lot and can’t truly understand context (for instance, figuring that “haircut” is something to do *before the birthday* and grouping it). The flexibility of GPT is preferred. But as a backup, one might research task-specific NLU like Google’s Dialogflow or other NLP frameworks. Those, however, would need training data and still likely not match GPT’s understanding out-of-the-box. In one example, a developer built an expense tracker that extracts key fields from voice commands using OpenAI and had to tune prompts for extraction ([What will be prompt for extracting entity extraction , summarization ...](https://community.openai.com/t/what-will-be-prompt-for-extracting-entity-extraction-summarization-and-classification/326354#:~:text=What%20will%20be%20prompt%20for,return%20the%20response%20like)). It reinforces that prompt-based extraction is the state of the art right now.
- **Expanding to Reminders/Dates:** In the example dictation, the user said “before then” (before the party tomorrow). This implies a **due date** for the tasks. Currently, our design doesn’t include capturing that, but it easily could. We could prompt GPT to also identify any deadlines or dates mentioned for tasks. E.g., “figure out a present **before tomorrow**” could be turned into a due date of tomorrow. If Telepathic supports due dates or reminders, this would be a neat enhancement. We’d have to be careful: casual speech might say “tomorrow” or “next Monday” which we can parse into a date with a date parser. GPT could output an ISO date or relative date for each task. Azure’s Cognitive Services has a LUIS/CLU that can do entity extraction like datetimes, but again GPT can do it in one go if asked. This might be something to iterate on – first get tasks, then later enrich them with due dates or priorities if mentioned (“high priority” etc. could be flagged since the user might say “really important”).
- **Testing Whisper vs. Azure in Practice:** It might be worthwhile to run a small **A/B test** or internal trial where some dictations go through Whisper, some through Azure, to compare transcript quality on realistic inputs. The difference might be negligible for our purposes, or Whisper might clearly be better. If Whisper consistently outperforms, we stick with it. If Azure is nearly as good and the streaming adds a lot of UX value, we might reconsider using Azure STT. The blog example by an AI enthusiast indicates Whisper’s dominance in quality ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision)), but every scenario is different.
- **Future UI Enhancements:** Once tasks are extracted, we might allow the user to further interact via voice. For example, after the tasks list is shown, the user could speak **commands** like “Delete the first one” or “Change the project name to Birthday Prep” and use voice for corrections. This would be an extension of voice interaction, basically voice-editing of the to-do list. Implementing that would require real-time speech command recognition and perhaps a different kind of parsing (commands instead of free-form dictation). That’s beyond the initial scope, but it shows how voice could be used more extensively in the app once the foundation is there.

## Conclusion and Recommendations  
Implementing the voice dictation feature in Telepathic will greatly enhance user productivity, allowing them to “brain-dump” their plans and let AI do the organizing. Our research suggests the following approach as optimal:

- **Use OpenAI Whisper API for transcription** of the recorded audio. It provides top-notch accuracy for converting speech to text, which is critical for reliable task extraction. We will record audio in the app (via Plugin.Maui.Audio or similar), likely as a single chunk when the user finishes speaking, and send it to the Whisper endpoint (or Azure OpenAI Whisper) for transcription. This keeps things simple and accurate, at the cost of not having real-time feedback. Given Whisper’s performance and moderate latency, this is a good starting point. *(If real-time feedback becomes a must-have, we can integrate Azure’s streaming STT in the future, possibly alongside Whisper.)*

- **Feed the transcript to an OpenAI GPT model** (using our existing OpenAI integration in .NET) with a carefully crafted prompt to extract tasks. We will instruct the model to ignore irrelevant details and output a structured list of projects and tasks, likely in JSON form. This leverages GPT’s understanding to interpret the user’s intent even if the speech was rambling or implicit about tasks. We should start with a strict prompt (and possibly use the structured output schema feature) to minimize errors. GPT-3.5 Turbo can be tried for speed, but we should test with GPT-4 for complex cases to ensure quality. In either case, have the user confirm the AI’s output.

- **Design the UI/UX** such that after speaking, the user sees the extracted tasks grouped by project in an editable format. They can correct any mistakes (which mitigates AI misinterpretation risk) before saving. Saving then creates those tasks/projects in the app’s data as if the user entered them manually. We should highlight that the list was AI-generated from their dictation (for transparency and perhaps delight). Over time, as the AI proves itself, users will trust it more and editing will be minimal.

- **Be mindful of technical considerations** like handling noise, ensuring we have mic permissions, showing appropriate loading states, and dealing with API failures. We will also monitor the feature’s usage to manage costs and scale. Caching isn’t applicable, but as usage grows we might consider whether running an on-premise instance of Whisper (if we have to scale a lot) is cost-effective. Currently, the API is cheap enough and avoids maintenance of ML infrastructure on our side.

- **Compare with Azure where relevant:** While our main path is OpenAI, we should keep an eye on Azure’s offerings. Azure Speech could be used if we find the need for streaming UX or if, for any reason, OpenAI’s service has limitations in certain languages our users need. Azure OpenAI could let us host the whole pipeline in an Azure data center with enterprise controls. We might also explore Azure’s conversational transcription for multi-speaker input if our feature scope ever widens to that. For now, these are secondary options, but it’s good to know we have them. The choice ultimately comes down to balancing *UX (real-time vs slight delay)* and *accuracy*. Our recommendation is to prioritize accuracy and simplicity (hence Whisper + GPT in batch mode) for the first iteration of this feature.

In conclusion, the combination of **OpenAI Whisper for speech-to-text** and **GPT-4 for language understanding** provides a powerful pipeline to turn unstructured voice dictation into structured to-do items. This approach has been validated by similar integrations – for example, apps that transcribe voice memos and use GPT to generate action items ([From Meeting Notes to Notion Tasks: AI Project Manager - Cohorte Projects](https://www.cohorte.co/blog/from-meeting-notes-to-notion-tasks-ai-project-manager#:~:text=Once%20the%20form%20is%20submitted%2C,story%2C%20and%20any%20helpful%20information)), as well as productivity hacks that use ChatGPT to turn meeting notes into task lists ([How I Use ChatGPT to Transform Messy Meeting Notes into Actionable Tasks (And Save 5 Hours Per Week) | by God of Prompt | Medium](https://medium.com/@godofpromptai/how-i-use-chatgpt-to-transform-messy-meeting-notes-into-actionable-tasks-and-save-5-hours-per-d64fb4378af8#:~:text=,to%20streamline%20their%20project%20management)) ([How I Use ChatGPT to Transform Messy Meeting Notes into Actionable Tasks (And Save 5 Hours Per Week) | by God of Prompt | Medium](https://medium.com/@godofpromptai/how-i-use-chatgpt-to-transform-messy-meeting-notes-into-actionable-tasks-and-save-5-hours-per-d64fb4378af8#:~:text=,tracking%20progress%20and%20ensuring%20accountability)). By following best practices in prompt design and keeping the user in the loop for review, Telepathic can offer a cutting-edge feature that feels almost “telepathic” – the user’s thoughts spoken aloud are seamlessly captured as organized tasks. With careful implementation and testing, this voice feature can greatly streamline how users plan their day, making the to-do list experience more natural and efficient.

## Sources

- OpenAI Whisper real-time limitations and chunking strategy ([How to use whisper model to transcribe audio in real time using Speech SDK? - Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/1637221/how-to-use-whisper-model-to-transcribe-audio-in-re#:~:text=For%20the%20real,time%20processing)) ([Possible to use for real-time / streaming tasks? · openai whisper · Discussion #2 · GitHub](https://github.com/openai/whisper/discussions/2#:~:text=))  
- OpenAI Whisper vs Azure Speech accuracy comparison (WER) ([OpenAI vs. Google vs. Azure: A Speech-to-Text Battle | by Stoyan Stoitsev | Medium](https://sstoitsev.medium.com/google-vs-azure-a-speech-to-text-battle-f740aa481e8e#:~:text=,applications%20that%20require%20high%20precision))  
- Azure Speech Service diarization and real-time support ([Real-time diarization quickstart - Speech service - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-stt-diarization#:~:text=In%20this%20quickstart%2C%20you%20run,particular%20part%20of%20transcribed%20speech)) ([Choosing between the OpenAI Whisper and Azure Speech models - Azure AI Services Video Tutorial | LinkedIn Learning, formerly Lynda.com](https://www.linkedin.com/learning/azure-ai-for-developers-azure-ai-speech/choosing-between-openai-whisper-vs-azure-speech-models#:~:text=language%20complexity%2C%20and%20domain%20specificity,or%20for%20applications%20where%20you%E2%80%A6))  
- Example of using GPT to extract tasks in JSON from text ([From Meeting Notes to Notion Tasks: AI Project Manager - Cohorte Projects](https://www.cohorte.co/blog/from-meeting-notes-to-notion-tasks-ai-project-manager#:~:text=client%20%3D%20OpenAI,transcript))  
- OpenAI Structured Output (JSON Schema) introduction ([Using Structured Outputs to Generate JSON responses with OpenAI | by Sebastian Jensen | Medialesson | Medium](https://medium.com/medialesson/using-structured-outputs-to-generate-json-responses-with-openai-e01f591b740f#:~:text=In%20some%20scenarios%2C%20receiving%20a,or%20producing%20invalid%20enum%20values))  
- OpenAI API pricing for Whisper (transcription) ([API model whisper - Real cost - OpenAI Developer Forum](https://community.openai.com/t/api-model-whisper-real-cost/469816#:~:text=API%20model%20whisper%20,So%20I)) and GPT models